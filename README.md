# ğŸš€ Building LLMs from Scratch

> **A comprehensive, step-by-step journey into building Large Language Models from the ground up**
>
> **Author:** Solomon Eshun

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Latest-red.svg)](https://pytorch.org/)

This repository contains the complete source code, explanations, and visualizations for the **"Building LLMs from Scratch"** series. Whether you're a beginner curious about how ChatGPT works or an experienced developer wanting to understand transformer architecture deeply, this series will guide you through every component step by step.

## ğŸ“š About This Series

This educational series breaks down the complexity of Large Language Models into digestible, hands-on tutorials. Each part builds upon the previous one, gradually constructing a complete transformer-based language model from scratch using PyTorch.

**ğŸ¯ Learning Objectives:**

- Understand the fundamental architecture of transformer models
- Implement each component (tokenization, embeddings, attention, etc.) from scratch
- Gain practical experience with PyTorch and deep learning concepts
- Learn best practices for training and evaluating language models
- Explore modern techniques used in state-of-the-art LLMs

**ğŸ‘¥ Target Audience:**

- Students and researchers in AI/ML
- Software engineers interested in NLP
- Anyone curious about how LLMs actually work
- Developers wanting to build custom language models

## ğŸ›£ï¸ Series Roadmap

| Part   | Topic                                  | Status         | Article                                                                                                                       | Code                                       |
| ------ | -------------------------------------- | -------------- | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
| **01** | The Complete Theoretical Foundation    | âœ… Complete    | [Medium](https://soloshun.medium.com/building-llms-from-scratch-part-1-the-complete-theoretical-foundation-e66b45b7f379)      | N/A                                        |
| **02** | Tokenization                           | âœ… Complete    | [Medium](https://medium.com/@soloshun/building-llms-from-scratch-part-2-tokenization-e0bf05d24094)                            | [Code](./src/part02_tokenization.py)       |
| **03** | Data Pipeline(Input-Target Pairs)      | âœ… Complete    | [Medium](https://soloshun.medium.com/building-llms-from-scratch-part-3-data-pipeline-4ef6eb7ad154)                            | [Code](./src/part03_dataloader.py)         |
| **04** | Token Embeddings & Positional Encoding | âœ… Complete    | [Medium](https://soloshun.medium.com/building-llms-from-scratch-part-4-embedding-layer-0803f6b8495b)                          | [Code](./src/part04_embeddings.py)         |
| **05** | Complete Data Preprocessing Pipeline   | âœ… Complete    | [Medium](https://soloshun.medium.com/building-llms-from-scratch-part-5-the-complete-data-preprocessing-pipeline-5247a8ee232a) | [Code](./src/part05_data_preprocessing.py) |
| **0-** | Self-Attention Mechanism               | ğŸ”„ In progress | [Medium](.)                                                                                                                   | [Code](./src/)                             |
| **0-** | Casual Attention                       | â³ Planned     | [Medium](.)                                                                                                                   | [Code](./src/)                             |
| **0-** | Multi-Head Attention                   | â³ Planned     | [Medium](.)                                                                                                                   | [Code](./src/)                             |
| **0-** | Transformer Blocks & Architecture      | â³ Planned     | [Medium](.)                                                                                                                   | [Code](./src/)                             |
| **0-** | Training Loop & Optimization           | â³ Planned     | [Medium](.)                                                                                                                   | [Code](./src/)                             |
| **0-** | Model Evaluation & Fine-tuning         | â³ Planned     | [Medium](.)                                                                                                                   | [Code](./src/)                             |

_Legend: âœ… Complete | ğŸ”„ In Progress | â³ Planned_

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Basic understanding of Python and neural networks
- Familiarity with PyTorch (helpful but not required)

### Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/soloeinsteinmit/llm-from-scratch.git
    cd llm-from-scratch
    ```

2.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the first code example (from Part 2):**
    ```bash
    python src/part02_tokenization.py
    ```

## ğŸ“ Repository Structure

```
llm-from-scratch/
â”œâ”€â”€ README.md                 # You are here!
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ LICENSE                   # MIT License
â”‚
â”œâ”€â”€ notebooks/                # Jupyter notebooks for interactive learning
â”‚   â”œâ”€â”€ part02_tokenization.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ animations/               # Manim visualizations and diagrams
â”‚   â””â”€â”€ part-02-WordTokenizationScene.mp4    # Generated animation files
â”‚
â”‚
â””â”€â”€ src/                      # Source code for each part
    â”œâ”€â”€ part02_tokenization.py
    â””â”€â”€ utils/                # Helper functions and utilities
```

## ğŸ“ How to Use This Repository

### For Learners

1.  **Start with Part 01** on Medium for the theoretical foundation.
2.  **Follow Part 02** and subsequent parts for hands-on coding.
3.  **Run the code** to see practical implementation.
4.  **Experiment** with the parameters and try modifications.
5.  **Check the notebooks** for interactive exploration.

### For Educators

- Use the code examples in your courses
- Reference the visualizations for explanations
- Adapt the materials for your curriculum
- Contribute improvements and additional examples

### For Researchers

- Use as a foundation for your own model implementations
- Reference the clean, well-documented code structure
- Build upon the base architecture for your experiments

## ğŸ¨ Visualizations

This series includes custom **Manim animations** that visualize complex concepts:

- ğŸ”„ **Attention mechanisms** - See how tokens "attend" to each other
- ğŸ“Š **Data flow** - Understand how information moves through the model
- ğŸ§® **Matrix operations** - Visualize the math behind transformers
- ğŸ“ˆ **Training dynamics** - Watch the model learn in real-time

_Animations are generated using [Manim](https://www.manim.community/) and available in the `animations/` directory._

## ğŸ¤ Contributing

We welcome contributions from the community! This is an open-source educational project aimed at making LLM understanding accessible to everyone.

**Ways to contribute:**

- ğŸ› Report bugs or suggest improvements
- ğŸ“ Improve documentation and explanations
- ğŸ¨ Create additional visualizations
- ğŸ”§ Add new features or optimizations
- ğŸŒ Translate content to other languages

<!-- Please read our [Contributing Guidelines](./CONTRIBUTING.md) and [Code of Conduct](./CODE_OF_CONDUCT.md) before submitting contributions. -->

## ğŸ“– Additional Resources

### Related Articles & Tutorials

- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer Paper
- [GPT-3 Paper](https://arxiv.org/abs/2005.14165) - Language Models are Few-Shot Learners

<!-- ### Recommended Resources

- **Books:** "Deep Learning" by Goodfellow, Bengio, and Courville
- **Courses:** CS224N (Stanford NLP), Fast.ai Deep Learning
- **Papers:** Start with the transformer paper, then explore GPT, BERT, and modern architectures -->

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Community:** Thanks to all contributors and learners who make this project better
- **Inspiration:** Built upon the excellent work of researchers and educators in the field
- **Tools:** Created with PyTorch, Manim, and lots of coffee â˜•

## ğŸ“± Connect & Follow

- ğŸ“ **Medium:** Follow the series on [Medium](https://soloshun.medium.com/)
- ğŸ’¼ **LinkedIn:** Connect and discuss on [LinkedIn](https://www.linkedin.com/in/solomon-eshun-788568317/)
- ğŸ™ **GitHub:** Star this repo and follow for updates

---

**â­ If you find this helpful, please give it a star! It helps others discover this resource.**

_Built with â¤ï¸ for the open-source community_
