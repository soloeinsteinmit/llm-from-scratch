---
title: "Building LLMs From Scratch (Part 5): The Complete Data Preprocessing Pipeline"
date: "2024-09-25"
tags:
  [
    "AI",
    "Machine Learning",
    "LLM",
    "PyTorch",
    "Deep Learning",
    "NLP",
    "Data Engineering",
  ]
series: "Building LLMs from Scratch"
---

# Building LLMs From Scratch (Part 5): The Complete Data Preprocessing Pipeline

Welcome back to the series! Over the last three practical parts, we have meticulously built the essential components for our LLM's data pipeline: a tokenizer, a data loader, and an embedding layer.

Each part was a deep dive into a specific concept. Now, it's time to zoom out and assemble them into a single, cohesive **data preprocessing pipeline**.

This article is a crucial milestone. By the end, we will have a complete, end-to-end process that takes raw, unstructured text and transforms it into a perfectly formatted, information-rich tensor that our future Transformer model can learn from.

Let's revisit our roadmap. Everything we've done so far completes the very first block of Stage 1.

![](../../images/stage1_1.png)

---

## Chapter 1: The Four Steps of Data Preprocessing

Our entire data preprocessing pipeline can be broken down into four logical steps. This is the journey each piece of text will take before it's ready for the model.

![](../../images/L12_preprocess.png)

1.  **Tokenization**: Convert the raw text into a sequence of integer token IDs.
2.  **Input-Target Pair Creation**: Generate chunks of text that the model will use to learn next-token prediction.
3.  **Token & Positional Embeddings**: Convert the token IDs into dense vectors that capture both semantic meaning and sequential order.
4.  **Batching**: Group multiple examples together into a "batch" for efficient training.

Let's briefly recap how we implemented each step.

---

## Chapter 2: Recap - From Text to Tokens (Step 1)

In [Part 2](https://medium.com/@soloshun/building-llms-from-scratch-part-2-tokenization-e0bf05d240945), we explored tokenization. We learned that simple word-based tokenizers struggle with large vocabularies and out-of-vocabulary words.

We chose to use a **Byte Pair Encoding (BPE)** tokenizer, specifically OpenAI's `tiktoken` library for GPT-2. This subword tokenization strategy gives us a great balance:

- A manageable vocabulary size.
- The ability to represent any word, even ones it has never seen before.

**Input**: `I HAD always thought...`
**Output**: `[40, 367, 2885, 1464, ...]`

---

## Chapter 3: Recap - From Tokens to Training Examples (Step 2)

A model doesn't learn from one giant stream of tokens. In [Part 3](https://medium.com/@soloshun/building-llms-from-scratch-part-3-data-pipeline-4ef6eb7ad154), we built a `Dataset` class to create meaningful training examples.

Using a **sliding window** approach, we slide across our token sequence to create `(input, target)` pairs.

- **Context Size**: The number of tokens the model sees as input.
- **Target**: The sequence of tokens the model must learn to predict (the input shifted by one).

This process generates thousands of examples, each teaching the model a small piece of the language puzzle.

**Input**: `[40, 367, 2885, 1464]`
**Output (Target)**: `[367, 2885, 1464, 1807]`

---

## Chapter 4: Recap - From IDs to Meaningful Vectors (Step 3)

In [Part 4](https://soloshun.medium.com/building-llms-from-scratch-part-4-the-embedding-layer-0803f6b8495b), we built the first layer of our neural network: the `GPTEmbedding` module. This crucial step converts the integer token IDs into information-rich vectors.

This process involves two key components:

1.  **Token Embeddings**: A learnable lookup table that maps each `token ID` to a dense vector representing its semantic meaning.
2.  **Positional Embeddings**: A second learnable lookup table that maps each `position index` (0, 1, 2, ...) to a dense vector representing its position in the sequence.

By adding these two vectors together, we create a final input embedding that contains both **what** the token is and **where** it is.

**Input**: `[40, 367, 2885, 1464]`
**Output**: A tensor of shape `[4, embedding_dim]` containing dense vectors.

---

## Chapter 5: Recap - Batching for Efficiency (Step 4)

Finally, our `DataLoader` from Part 3 takes all the individual `(input, target)` pairs generated by the `Dataset` and groups them into batches.

Training one example at a time is computationally inefficient. By processing a **batch** of examples (e.g., 8 or 16 at a time), we can leverage the power of modern GPUs for parallel processing, dramatically speeding up the training process.

The `DataLoader` also handles shuffling the data, which is critical for helping the model generalize instead of just memorizing the order of the text.

**Input**: Multiple `(input, target)` pairs.
**Output**: A single batch containing a tensor for inputs and a tensor for targets. The final input tensor has the shape `[batch_size, context_size, embedding_dim]`.

---

## Chapter 6: The Pipeline in Action (Code)

Now, let's see how these components work together in a single, streamlined piece of code. This snippet demonstrates our entire pipeline, from loading raw text to producing the final model-ready tensor.

```python
import torch
from part03_dataloader import create_dataloader_v1
from part04_embeddings import GPTEmbedding

# -- HYPERPARAMETERS --
VOCAB_SIZE = 50257
EMB_DIM = 256
CONTEXT_SIZE = 4
BATCH_SIZE = 8

# 1. Load Raw Text
with open("../data/the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

# 2. Create DataLoader (Handles Tokenization & Input-Target Pairs)
dataloader = create_dataloader_v1(
    raw_text,
    batch_size=BATCH_SIZE,
    max_length=CONTEXT_SIZE,
    stride=CONTEXT_SIZE,
    shuffle=False
)

# 3. Initialize the Embedding Layer
embedding_layer = GPTEmbedding(VOCAB_SIZE, EMB_DIM, CONTEXT_SIZE)

# -- Run the Pipeline --

# 4. Get one batch of data
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# 5. Pass inputs through the embedding layer
model_ready_inputs = embedding_layer(inputs)

# 6. Inspect the final output
print("Input Token IDs (from DataLoader):")
print(f"Shape: {inputs.shape}")
print(inputs)
print("\\n--------------------->")
print("Model-Ready Tensor (after Embedding Layer):")
print(f"Shape: {model_ready_inputs.shape}")
print(model_ready_inputs)
```

As you can see, our modular components from previous parts fit together perfectly to execute the entire preprocessing pipeline with just a few lines of code.

---

## Conclusion & What's Next

And there we have it! We have successfully assembled our entire data preprocessing pipeline. We now have a robust, end-to-end process that can take any raw text file and produce batches of model-ready, information-rich tensors.

This completes the foundational "Data Preparation" stage. We have built the system that will feed our LLM.

Now, we can finally turn our attention to the heart of the model itself. In the next part, we will begin building the core of the LLM architecture.

**In Part 6, we will implement the revolutionary self-attention mechanism from scratch.** This is the engine that allows the model to understand context by weighing the importance of different words in a sequence, and it's where the magic of the Transformer truly begins.

See you in the next one!

---

## üßë‚Äçüíª Full Code

You can find the complete, interactive code that consolidates this entire pipeline in our GitHub repository:

- **Jupyter Notebook**: [llm-from-scratch/notebooks/part05_data_preprocessing.ipynb](https://github.com/soloeinsteinmit/llm-from-scratch/blob/main/notebooks/part05_data_preprocessing.ipynb)
- **Python Script**: [llm-from-scratch/src/part05_data_preprocessing.py](https://github.com/soloeinsteinmit/llm-from-scratch/blob/main/src/part05_data_preprocessing.py)
