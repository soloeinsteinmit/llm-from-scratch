{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building LLMs From Scratch (Part 2): The Power of Tokenization\n",
        "\n",
        "Welcome to Part 2 of our **\"Building LLMs from Scratch\"** series! \n",
        "\n",
        "In [Part 1](https://soloshun.medium.com/building-llms-from-scratch-part-1-the-complete-theoretical-foundation-e66b45b7f379), we established the theoretical foundation. Now it's time to get practical and take our first step into code.\n",
        "\n",
        "## What We'll Learn Today\n",
        "\n",
        "1. **Different tokenization strategies** and their trade-offs\n",
        "2. **Build a simple tokenizer** from scratch using Python\n",
        "3. **Understand Byte Pair Encoding (BPE)** - the algorithm behind modern LLMs\n",
        "4. **Use professional tokenizers** like OpenAI's `tiktoken`\n",
        "\n",
        "Let's start coding! üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "**üìñ This notebook accompanies the Medium article:** [Building LLMs From Scratch (Part 2): The Power of Tokenization]\n",
        "\n",
        "**üìÇ Find the clean Python script version:** [src/part02_tokenization.py](../src/part02_tokenization.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 1: The Tokenization Dilemma\n",
        "\n",
        "Before a Large Language Model can understand text, it needs to convert that text into numbers. This process is called **tokenization**.\n",
        "\n",
        "But how should we split a sentence? There are three main approaches:\n",
        "\n",
        "### 1. Word-Based Tokenization\n",
        "Split by spaces and punctuation. Simple but has problems with unknown words.\n",
        "\n",
        "### 2. Character-Based Tokenization  \n",
        "Split into individual characters. No unknown words, but loses word meaning.\n",
        "\n",
        "### 3. Sub-word Tokenization ‚≠ê\n",
        "The modern approach! Keep common words whole, split rare words into meaningful parts.\n",
        "\n",
        "Let's see these in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see the difference between tokenization approaches\n",
        "text = \"My hobby is playing cricket\"\n",
        "\n",
        "# 1. Word-based tokenization\n",
        "word_tokens = text.split()\n",
        "print(\"Word-based:\", word_tokens)\n",
        "print(\"Vocabulary size would be HUGE with all possible words!\\n\")\n",
        "\n",
        "# 2. Character-based tokenization  \n",
        "char_tokens = list(text)\n",
        "print(\"Character-based:\", char_tokens)\n",
        "print(\"Very long sequences, meaning is lost!\\n\")\n",
        "\n",
        "# 3. Sub-word tokenization (we'll build this!)\n",
        "print(\"Sub-word tokenization:\")\n",
        "print(\"- Common words like 'is' stay as one token\")\n",
        "print(\"- Rare words like 'snowboarding' might become ['snow', 'board', 'ing']\")\n",
        "print(\"- Best of both worlds! ‚ú®\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2: Building Our First Tokenizer From Scratch\n",
        "\n",
        "Now let's build a simple word-based tokenizer to understand the mechanics. We'll use \"The Verdict\" by Edith Wharton as our training text.\n",
        "\n",
        "### Step 1: Load the Text Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load our text data\n",
        "try:\n",
        "    with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "    print(f\"‚úÖ Successfully loaded text! Total characters: {len(raw_text)}\")\n",
        "    print(f\"First 100 characters:\\n'{raw_text[:100]}'\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå File not found. Make sure 'the-verdict.txt' is in the '../data/' folder\")\n",
        "    # For demonstration, let's use a sample text\n",
        "    raw_text = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that he had been caught.\"\"\"\n",
        "    print(f\"Using sample text instead: '{raw_text[:80]}...'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Split Text into Tokens\n",
        "\n",
        "We'll use regular expressions to split text by spaces and punctuation, while keeping punctuation as separate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# First, let's see how regex splitting works on a simple example\n",
        "example = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', example)\n",
        "print(\"Raw split result:\", result)\n",
        "\n",
        "# Clean up empty strings and whitespace\n",
        "cleaned = [item.strip() for item in result if item.strip()]\n",
        "print(\"Cleaned tokens:\", cleaned)\n",
        "print(\"‚ú® Notice how punctuation becomes separate tokens!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's apply this to our full text\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "print(f\"Total tokens: {len(preprocessed)}\")\n",
        "print(\"First 20 tokens:\", preprocessed[:20])\n",
        "\n",
        "# Build our vocabulary (mapping tokens to IDs)\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_tokens)\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")\n",
        "print(\"First 10 vocabulary entries:\")\n",
        "for i, (token, id_) in enumerate(list(vocab.items())[:10]):\n",
        "    print(f\"  '{token}' -> {id_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create the Tokenizer Class\n",
        "\n",
        "Now let's wrap our logic in a reusable class with `encode` and `decode` methods:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab  # token -> ID\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}  # ID -> token\n",
        "        \n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to a list of token IDs\"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "    \n",
        "    def decode(self, ids):\n",
        "        \"\"\"Convert token IDs back to text\"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Clean up spacing around punctuation\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "# Test our tokenizer!\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "test_text = \"Hello, world. This is a test!\"\n",
        "\n",
        "encoded = tokenizer.encode(test_text)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"Original:  '{test_text}'\")\n",
        "print(f\"Encoded:   {encoded}\")\n",
        "print(f\"Decoded:   '{decoded}'\")\n",
        "print(f\"Success:   {test_text == decoded}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Problem: Unknown Words! ‚ùå\n",
        "\n",
        "Our tokenizer works great... until it encounters a word not in its vocabulary:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try encoding a word that's not in our vocabulary\n",
        "try:\n",
        "    unknown_text = \"Hello, do you like pizza?\"  # \"pizza\" probably isn't in our vocabulary\n",
        "    tokenizer.encode(unknown_text)\n",
        "    print(\"‚úÖ Encoded successfully!\")\n",
        "except KeyError as e:\n",
        "    print(f\"‚ùå KeyError: {e}\")\n",
        "    print(\"Our tokenizer crashes when it sees unknown words!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 3: Handling Unknown Words with Special Tokens\n",
        "\n",
        "To fix this, we'll add special tokens to our vocabulary:\n",
        "\n",
        "- `<|unk|>`: Represents unknown words\n",
        "- `<|endoftext|>`: Separates different documents/passages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create improved vocabulary with special tokens\n",
        "all_tokens_v2 = sorted(list(set(preprocessed)))\n",
        "all_tokens_v2.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab_v2 = {token: integer for integer, token in enumerate(all_tokens_v2)}\n",
        "\n",
        "print(f\"New vocabulary size: {len(vocab_v2)}\")\n",
        "print(\"Special tokens added:\")\n",
        "print(f\"  '<|endoftext|>' -> {vocab_v2['<|endoftext|>']}\")\n",
        "print(f\"  '<|unk|>' -> {vocab_v2['<|unk|>']}\")\n",
        "\n",
        "class SimpleTokenizerV2(SimpleTokenizerV1):\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to token IDs, handling unknown words\"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        \n",
        "        # Replace unknown tokens with <|unk|>\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int else \"<|unk|>\" \n",
        "            for item in preprocessed\n",
        "        ]\n",
        "        \n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "# Test the improved tokenizer\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab_v2)\n",
        "\n",
        "# Test with known words\n",
        "test1 = \"Hello, world!\"\n",
        "print(f\"Known words: '{test1}' -> {tokenizer_v2.encode(test1)}\")\n",
        "\n",
        "# Test with unknown words  \n",
        "test2 = \"Hello, do you like pizza?\"\n",
        "encoded_with_unknown = tokenizer_v2.encode(test2)\n",
        "decoded_with_unknown = tokenizer_v2.decode(encoded_with_unknown)\n",
        "\n",
        "print(f\"With unknown: '{test2}'\")\n",
        "print(f\"Encoded: {encoded_with_unknown}\")\n",
        "print(f\"Decoded: '{decoded_with_unknown}'\")\n",
        "print(\"‚úÖ No more crashes!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 4: The Industry Standard - Byte Pair Encoding (BPE)\n",
        "\n",
        "Our simple tokenizer works, but modern LLMs use **Byte Pair Encoding (BPE)** - a subword algorithm that elegantly solves the vocabulary size vs. unknown word trade-off.\n",
        "\n",
        "### How BPE Works:\n",
        "1. Start with character-level tokens\n",
        "2. Iteratively merge the most frequent consecutive pair\n",
        "3. Build a vocabulary of common subwords\n",
        "\n",
        "Let's see BPE in action using OpenAI's `tiktoken` library:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's install tiktoken (run this if not already installed)\n",
        "# !pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Load GPT-2's BPE tokenizer\n",
        "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(\"Available encodings:\", tiktoken.list_encoding_names())\n",
        "print(f\"GPT-2 tokenizer loaded!\")\n",
        "\n",
        "# Test with the same text\n",
        "text = \"Hello, do you like tea? <|endoftext|> Akwirw ier\"\n",
        "\n",
        "# Encode\n",
        "integers = gpt2_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(f\"\\nOriginal: '{text}'\")\n",
        "print(f\"Encoded:  {integers}\")\n",
        "\n",
        "# Decode\n",
        "decoded = gpt2_tokenizer.decode(integers)\n",
        "print(f\"Decoded:  '{decoded}'\")\n",
        "\n",
        "print(f\"\\n‚ú® Notice: BPE breaks unknown 'Akwirw ier' into subword tokens!\")\n",
        "print(\"This is much better than replacing it with <|unk|>\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's See BPE Magic in Action! ‚ú®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare our simple tokenizer vs. GPT-2's BPE tokenizer\n",
        "test_words = [\"tokenization\", \"snowboarding\", \"artificialintelligence\", \"pizza\"]\n",
        "\n",
        "print(\"üîÑ Tokenization Comparison:\\n\")\n",
        "print(f\"{'Word':<20} {'Our Tokenizer':<15} {'GPT-2 BPE':<30}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for word in test_words:\n",
        "    # Our tokenizer (will use <|unk|> for unknown words)\n",
        "    try:\n",
        "        our_result = tokenizer_v2.encode(word)\n",
        "        our_tokens = [tokenizer_v2.int_to_str[id_] for id_ in our_result]\n",
        "    except:\n",
        "        our_tokens = [\"<|unk|>\"]\n",
        "    \n",
        "    # GPT-2 BPE tokenizer\n",
        "    gpt2_ids = gpt2_tokenizer.encode(word)\n",
        "    gpt2_tokens = [gpt2_tokenizer.decode([id_]) for id_ in gpt2_ids]\n",
        "    \n",
        "    print(f\"{word:<20} {str(our_tokens):<15} {str(gpt2_tokens):<30}\")\n",
        "\n",
        "print(\"\\nüéØ Key Insight: BPE breaks unknown words into meaningful subword pieces!\")\n",
        "print(\"   This allows the model to understand patterns even in words it's never seen.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Summary & What We've Learned\n",
        "\n",
        "Congratulations! You've just built your first tokenizer and understand the fundamentals of modern tokenization.\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Tokenization is crucial** - It's the bridge between human text and machine numbers\n",
        "2. **Word-level is simple** but has vocabulary explosion problems\n",
        "3. **Character-level solves vocabulary** but loses meaning and creates long sequences  \n",
        "4. **Subword (BPE) is the sweet spot** - handles unknown words gracefully while keeping sequences manageable\n",
        "5. **Professional tokenizers like tiktoken** are highly optimized and ready for production use\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "Now that we can convert text into token sequences, we need to prepare this data for training our LLM. \n",
        "\n",
        "**In Part 3**, we'll build a **data loader** that creates input-target pairs from our token sequences - the exact format our model needs for training!\n",
        "\n",
        "---\n",
        "\n",
        "üîó **Find the complete code:**\n",
        "- **This notebook**: `notebooks/part02_tokenization.ipynb`\n",
        "- **Python script**: `src/part02_tokenization.py`  \n",
        "- **GitHub repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
        "\n",
        "üìù **Read the full article**: [Building LLMs From Scratch (Part 2): The Power of Tokenization](https://soloshun.medium.com/building-llms-from-scratch-part-2-the-power-of-tokenization)\n",
        "\n",
        "Happy coding! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
