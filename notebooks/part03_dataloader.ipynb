{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building LLMs From Scratch (Part 3): Crafting the Data Pipeline\n",
        "\n",
        "Welcome to Part 3 of our **\"Building LLMs from Scratch\"** series! \n",
        "\n",
        "In [Part 1](https://soloshun.medium.com/building-llms-from-scratch-part-1-the-complete-theoretical-foundation-e66b45b7f379), we built the theoretical foundation. In [Part 2](https://soloshun.medium.com/building-llms-from-scratch-part-2-the-power-of-tokenization), we learned how to convert raw text into tokens.\n",
        "\n",
        "Now it's time to tackle a crucial step: **How do we feed this data to our model?**\n",
        "\n",
        "## What We'll Learn Today\n",
        "\n",
        "1. **The core concept**: Why LLMs need input-target pairs\n",
        "2. **Key parameters**: Context size and stride - the building blocks of our data pipeline  \n",
        "3. **PyTorch implementation**: Build a custom `Dataset` and `DataLoader`\n",
        "4. **See it in action**: Watch our pipeline create training batches\n",
        "\n",
        "Let's dive in! üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "**üìñ This notebook accompanies the Medium article:** [Building LLMs From Scratch (Part 3): Crafting the Data Pipeline]\n",
        "\n",
        "**üìÇ Find the clean Python script version:** [src/part03_dataloader.py](../src/part03_dataloader.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 1: The Core Task - Next Token Prediction\n",
        "\n",
        "A GPT-style LLM has **one fundamental job**: Given a sequence of tokens, predict what token comes next.\n",
        "\n",
        "To train the model to do this, we need to show it millions of examples where:\n",
        "- **Input**: A sequence of tokens (e.g., `[40, 367, 2885, 1464]`)\n",
        "- **Target**: The next token that should follow (e.g., `1807`)\n",
        "\n",
        "But there's a catch: we don't just want to predict one token at a time. We want to make predictions at **every position** in the sequence. This is much more efficient for training.\n",
        "\n",
        "Let's see what this looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's start with a simple example\n",
        "import tiktoken\n",
        "\n",
        "# Load our tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Example text \n",
        "text = \"I HAD always thought Jack Gisburn rather a cheap genius\"\n",
        "tokens = tokenizer.encode(text)\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's see how we create input-target pairs\n",
        "context_size = 8  # How many tokens to use as input\n",
        "\n",
        "# Create input and target sequences\n",
        "input_tokens = tokens[:context_size]\n",
        "target_tokens = tokens[1:context_size+1]  # Shifted by one position\n",
        "\n",
        "print(\"üéØ Input-Target Pair Example:\")\n",
        "print(f\"Input:  {input_tokens}\")\n",
        "print(f\"Target: {target_tokens}\")\n",
        "print()\n",
        "\n",
        "# Let's see what this means for each prediction task\n",
        "print(\"üìù Individual Prediction Tasks:\")\n",
        "for i in range(len(input_tokens)):\n",
        "    context = input_tokens[:i+1]\n",
        "    target = target_tokens[i]\n",
        "    context_text = tokenizer.decode(context)\n",
        "    target_text = tokenizer.decode([target])\n",
        "    print(f\"'{context_text}' ‚Üí '{target_text}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2: Key Concepts - Context Size & Stride\n",
        "\n",
        "Now that we understand the basic idea, let's explore the two crucial parameters that control our data pipeline.\n",
        "\n",
        "### Context Size (max_length)\n",
        "The **context size** determines how many tokens the model looks at when making a prediction. It's like the model's \"attention span.\"\n",
        "\n",
        "### Stride  \n",
        "The **stride** controls how we slide our window across the text to create the next chunk:\n",
        "- **stride = 1**: Maximum overlap, more training examples, computationally intensive\n",
        "- **stride = context_size**: No overlap, fewer examples, faster training\n",
        "\n",
        "Let's visualize this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's demonstrate different stride values\n",
        "def show_sliding_window(tokens, context_size, stride, max_examples=5):\n",
        "    print(f\"üìä Sliding Window: context_size={context_size}, stride={stride}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    count = 0\n",
        "    for i in range(0, len(tokens) - context_size, stride):\n",
        "        if count >= max_examples:\n",
        "            print(\"... (and more)\")\n",
        "            break\n",
        "            \n",
        "        input_chunk = tokens[i:i + context_size]\n",
        "        target_chunk = tokens[i + 1:i + context_size + 1]\n",
        "        \n",
        "        print(f\"Window {count+1}: Input={input_chunk}, Target={target_chunk}\")\n",
        "        count += 1\n",
        "    \n",
        "    total_chunks = len(range(0, len(tokens) - context_size, stride))\n",
        "    print(f\"üìà Total chunks created: {total_chunks}\")\n",
        "    print()\n",
        "\n",
        "# Compare different stride values\n",
        "show_sliding_window(tokens, context_size=4, stride=1)\n",
        "show_sliding_window(tokens, context_size=4, stride=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 3: Building the PyTorch Dataset\n",
        "\n",
        "Now let's implement this logic using PyTorch's `Dataset` class. This class needs three methods:\n",
        "\n",
        "1. `__init__`: Set up the data (tokenize and create all chunks)\n",
        "2. `__len__`: Return the number of chunks\n",
        "3. `__getitem__`: Return a specific chunk by index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPTDatasetV1 class created!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        \n",
        "        # 1. Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        \n",
        "        # 2. Use a sliding window to create input-target chunks\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i : i + max_length]\n",
        "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "            \n",
        "    def __len__(self):\n",
        "        # 3. Return the total number of chunks\n",
        "        return len(self.input_ids)\n",
        "       \n",
        "    def __getitem__(self, idx):\n",
        "        # 4. Return a single input-target pair\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "print(\"‚úÖ GPTDatasetV1 class created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 4: Creating the DataLoader\n",
        "\n",
        "The `DataLoader` takes our `Dataset` and handles:\n",
        "- **Batching**: Groups multiple examples together\n",
        "- **Shuffling**: Randomizes the order for better training\n",
        "- **Parallel processing**: Uses multiple CPU cores for speed\n",
        "\n",
        "Let's create a utility function to set this up:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloader_v1(\n",
        "    txt, \n",
        "    batch_size=4,\n",
        "    max_length=256,\n",
        "    stride=128,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        "):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    \n",
        "    # Create the dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    \n",
        "    # Create the dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    \n",
        "    print(f\"üì¶ DataLoader created:\")\n",
        "    print(f\"   Dataset size: {len(dataset)} chunks\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Number of batches: {len(dataloader)}\")\n",
        "    \n",
        "    return dataloader\n",
        "\n",
        "print(\"‚úÖ create_dataloader_v1 function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 5: Testing with Real Data\n",
        "\n",
        "Let's load our text data and see our pipeline in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the text data\n",
        "try:\n",
        "    with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "    print(f\"‚úÖ Successfully loaded text! Total characters: {len(raw_text)}\")\n",
        "    print(f\"First 100 characters: '{raw_text[:100]}'\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå File not found. Make sure 'the-verdict.txt' is in the '../data/' folder\")\n",
        "    # For demonstration, let's use a sample text\n",
        "    raw_text = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that he had been caught.\"\"\"\n",
        "    print(f\"Using sample text instead: '{raw_text[:80]}...'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small dataloader for easy inspection\n",
        "print(\"üî¨ Creating a small dataloader for inspection...\")\n",
        "small_dataloader = create_dataloader_v1(\n",
        "    raw_text, \n",
        "    batch_size=8, \n",
        "    max_length=4, \n",
        "    stride=4, \n",
        "    shuffle=False  # Keep order for easier understanding\n",
        ")\n",
        "\n",
        "# Get the first batch\n",
        "data_iter = iter(small_dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(f\"\\nüéØ First Batch:\")\n",
        "print(f\"Inputs shape:  {inputs.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"\\nInputs:\\n{inputs}\")\n",
        "print(f\"\\nTargets:\\n{targets}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's decode a few examples to see the actual text\n",
        "print(\"üìñ Decoded Examples:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i in range(min(3, inputs.shape[0])):  # Show first 3 examples\n",
        "    input_text = tokenizer.decode(inputs[i].tolist())\n",
        "    target_text = tokenizer.decode(targets[i].tolist())\n",
        "    \n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"  Input:  '{input_text}'\")\n",
        "    print(f\"  Target: '{target_text}'\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's create a more realistic dataloader\n",
        "print(\"üöÄ Creating a realistic dataloader for training...\")\n",
        "training_dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=4,\n",
        "    max_length=256,\n",
        "    stride=128,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Show what a training batch looks like\n",
        "data_iter = iter(training_dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(f\"\\nüéØ Training Batch:\")\n",
        "print(f\"Inputs shape:  {inputs.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"Batch size: {inputs.shape[0]}\")\n",
        "print(f\"Sequence length: {inputs.shape[1]}\")\n",
        "\n",
        "# Show a snippet of the first example\n",
        "print(f\"\\nFirst example (first 20 tokens):\")\n",
        "print(f\"Input:  {inputs[0][:20].tolist()}\")\n",
        "print(f\"Target: {targets[0][:20].tolist()}\")\n",
        "print(f\"Text:   '{tokenizer.decode(inputs[0][:20].tolist())}'...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Summary & What We've Accomplished\n",
        "\n",
        "Congratulations! You've just built a complete data pipeline for training LLMs. Here's what we've achieved:\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Understanding the Task**: LLMs learn by predicting the next token at every position in a sequence\n",
        "2. **Critical Parameters**: \n",
        "   - **Context Size**: Controls how much text the model sees at once\n",
        "   - **Stride**: Controls overlap between training examples\n",
        "3. **PyTorch Implementation**: We built a custom `Dataset` and `DataLoader` that can handle any text data\n",
        "4. **Efficiency**: Our pipeline automatically creates thousands of training examples from raw text\n",
        "\n",
        "### The Magic ‚ú®\n",
        "\n",
        "Look at what we've created: our `DataLoader` takes raw text and automatically generates perfectly formatted training batches where:\n",
        "- Each input sequence contains `max_length` tokens\n",
        "- Each target sequence is the input shifted by one position\n",
        "- The model will learn to predict the next token at every position\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "Now that we can feed data to our model, we need to build the model itself!\n",
        "\n",
        "**In Part 4**, we'll tackle **embeddings**‚Äîhow we convert our token IDs into meaningful vectors that capture semantic relationships. This is where the real magic of understanding begins.\n",
        "\n",
        "---\n",
        "\n",
        "üîó **Find the complete code:**\n",
        "- **This notebook**: `notebooks/part03_dataloader.ipynb`\n",
        "- **Python script**: `src/part03_dataloader.py`  \n",
        "- **GitHub repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
        "\n",
        "üìù **Read the full article**: [Building LLMs From Scratch (Part 3): Crafting the Data Pipeline]\n",
        "\n",
        "Happy coding! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sophia-dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
