{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building LLMs From Scratch (Part 4): The Embedding Layer\n",
        "\n",
        "Welcome to the notebook for Part 4 of the series! Here, we'll implement the concepts discussed in the Medium article, building our model's first and most critical layer: the embedding layer.\n",
        "\n",
        "This layer is responsible for turning meaningless token IDs into information-rich vectors that capture both **semantic meaning** and **sequential order**.\n",
        "\n",
        "### üîó Quick Links\n",
        "- **Medium Article**: [Part 4: The Embedding Layer](https://soloshun.medium.com/building-llms-from-scratch-part-4-embedding-layer-0803f6b8495b)\n",
        "- **GitHub Repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, let's import the necessary libraries. We'll need `torch` for our neural network components and the `create_dataloader_v1` function we built in Part 3 to feed us data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import tiktoken\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the system path to allow imports\n",
        "sys.path.insert(0, '../')\n",
        "\n",
        "from src.part03_dataloader import create_dataloader_v1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Token Embeddings\n",
        "\n",
        "A token embedding layer is essentially a lookup table (a matrix) where we store a dense vector for every token in our vocabulary. \n",
        "\n",
        "- **Shape**: `[vocab_size, embedding_dim]`\n",
        "- **Function**: It maps a token ID (an integer) to its corresponding vector representation.\n",
        "\n",
        "Let's define our model's hyperparameters and create the layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the hyperparameters for our model\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size (same as GPT-2)\n",
        "    \"context_size\": 256,    # Context length\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "# For this notebook, we'll use smaller values to make things manageable\n",
        "vocab_size = GPT_CONFIG_124M[\"vocab_size\"]  # Keep the real vocab size\n",
        "embedding_dim = 256  # Smaller embedding dimension for demonstration\n",
        "context_size = 4     # Tiny context size for easy inspection\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  - Vocabulary Size: {vocab_size:,}\")\n",
        "print(f\"  - Embedding Dimension: {embedding_dim}\")\n",
        "print(f\"  - Context Size: {context_size}\")\n",
        "print(f\"  - Embedding Matrix Shape: [{vocab_size:,}, {embedding_dim}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the token embedding layer\n",
        "torch.manual_seed(123)\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "print(\"‚úÖ Token Embedding Layer Created!\")\n",
        "print(f\"Shape of the embedding matrix: {token_embedding_layer.weight.shape}\")\n",
        "print(f\"This matrix contains {token_embedding_layer.weight.numel():,} learnable parameters!\")\n",
        "\n",
        "# Let's examine a few token embeddings\n",
        "print(\"\\nüîç Examining specific token embeddings:\")\n",
        "print(f\"Token ID 0 embedding shape: {token_embedding_layer.weight[0].shape}\")\n",
        "print(f\"Token ID 0 embedding (first 10 values): {token_embedding_layer.weight[0][:10]}\")\n",
        "\n",
        "# Demonstrate that we can look up embeddings for specific tokens\n",
        "sample_token_ids = torch.tensor([0, 1, 2])\n",
        "sample_embeddings = token_embedding_layer(sample_token_ids)\n",
        "print(f\"\\nSample embeddings for tokens [0, 1, 2] shape: {sample_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing a Batch of Data\n",
        "\n",
        "Now, let's see how this layer processes a batch of token IDs from our `DataLoader`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load some text data\n",
        "with open(\"../data/the-verdict.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# 2. Create a dataloader with a small context size\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, \n",
        "    batch_size=8, \n",
        "    max_length=context_size,\n",
        "    stride=context_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 3. Get one batch of data\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Token IDs (inputs):\\\\n\", inputs)\n",
        "print(\"\\\\nShape of inputs:\", inputs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Pass the token IDs through the embedding layer\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "\n",
        "print(\"üéØ Token Embeddings Results:\")\n",
        "print(f\"Shape of token embeddings: {token_embeddings.shape}\")\n",
        "print(\"Each token ID has been converted to a 256-dimensional vector!\")\n",
        "\n",
        "# Let's examine one sample in detail\n",
        "print(f\"\\nüîç Examining first sample:\")\n",
        "print(f\"Token IDs: {inputs[0]}\")\n",
        "print(f\"Token embeddings shape for first sample: {token_embeddings[0].shape}\")\n",
        "print(f\"First token embedding (first 10 values): {token_embeddings[0][0][:10]}\")\n",
        "\n",
        "# Show that different tokens have different embeddings\n",
        "print(f\"\\nüìä Embedding differences:\")\n",
        "print(f\"Are embeddings for tokens {inputs[0][0]} and {inputs[0][1]} the same? {torch.equal(token_embeddings[0][0], token_embeddings[0][1])}\")\n",
        "\n",
        "# Calculate the distance between two token embeddings\n",
        "distance = torch.norm(token_embeddings[0][0] - token_embeddings[0][1])\n",
        "print(f\"L2 distance between first two token embeddings: {distance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the input tensor of shape `[8, 4]` has been transformed into an output tensor of shape `[8, 4, 256]`. Each of the original integer token IDs is now represented by a 256-dimensional vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Positional Embeddings\n",
        "\n",
        "Token embeddings capture meaning but lose the order of the words. To fix this, we introduce **positional embeddings**.\n",
        "\n",
        "This is another learnable lookup table, but this time it encodes the *position* of a token in the sequence (0, 1, 2, ...), not its meaning.\n",
        "\n",
        "- **Shape**: `[context_size, embedding_dim]`\n",
        "- **Function**: It maps a position index to a vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the positional embedding layer\n",
        "pos_embedding_layer = torch.nn.Embedding(context_size, embedding_dim)\n",
        "\n",
        "print(\"‚úÖ Positional Embedding Layer Created!\")\n",
        "print(f\"Shape of the positional embedding matrix: {pos_embedding_layer.weight.shape}\")\n",
        "print(f\"This matrix contains {pos_embedding_layer.weight.numel():,} learnable parameters\")\n",
        "\n",
        "# Show what each position embedding looks like\n",
        "print(f\"\\nüîç Position embeddings:\")\n",
        "for i in range(context_size):\n",
        "    pos_emb = pos_embedding_layer.weight[i]\n",
        "    print(f\"Position {i} embedding (first 10 values): {pos_emb[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the positional embeddings for our sequence of length 4, we pass the indices `0, 1, 2, 3` to this layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the positional embeddings\n",
        "pos_ids = torch.arange(context_size)\n",
        "print(\"üî¢ Position IDs:\", pos_ids)\n",
        "\n",
        "pos_embeddings = pos_embedding_layer(pos_ids)\n",
        "print(f\"\\nüéØ Positional Embeddings Results:\")\n",
        "print(f\"Shape of positional embeddings: {pos_embeddings.shape}\")\n",
        "print(f\"These represent position information for {context_size} positions\")\n",
        "\n",
        "# Show that position embeddings are different for different positions\n",
        "print(f\"\\nüìä Position differences:\")\n",
        "print(f\"Are position 0 and position 1 embeddings the same? {torch.equal(pos_embeddings[0], pos_embeddings[1])}\")\n",
        "\n",
        "# Calculate distance between position embeddings\n",
        "pos_distance = torch.norm(pos_embeddings[0] - pos_embeddings[1])\n",
        "print(f\"L2 distance between position 0 and position 1 embeddings: {pos_distance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Combining Token and Positional Embeddings\n",
        "\n",
        "The final input to the transformer is the sum of the token and positional embeddings. PyTorch's **broadcasting** feature makes this easy. When we add the `[8, 4, 256]` token embeddings to the `[4, 256]` positional embeddings, PyTorch automatically expands the positional embeddings to match the batch dimension.\n",
        "\n",
        "`Input Embeddings = Token Embeddings + Positional Embeddings`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the two embeddings together\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "\n",
        "print(\"üîÑ Broadcasting and Addition:\")\n",
        "print(f\"Token Embeddings shape:     {token_embeddings.shape}\")\n",
        "print(f\"Positional Embeddings shape: {pos_embeddings.shape}\")\n",
        "print(f\"Final Input Embeddings shape: {input_embeddings.shape}\")\n",
        "\n",
        "print(\"\\n‚ú® PyTorch automatically broadcast the positional embeddings!\")\n",
        "print(\"The [4, 256] positional tensor was expanded to [8, 4, 256] to match the token embeddings\")\n",
        "\n",
        "# Verify that the addition worked correctly\n",
        "print(f\"\\nüîç Verification:\")\n",
        "print(f\"Original token embedding for first token: {token_embeddings[0][0][:5]}\")\n",
        "print(f\"Position 0 embedding: {pos_embeddings[0][:5]}\")\n",
        "print(f\"Combined embedding for first token: {input_embeddings[0][0][:5]}\")\n",
        "print(f\"Manual addition check: {(token_embeddings[0][0] + pos_embeddings[0])[:5]}\")\n",
        "\n",
        "# Show that all batches got the same positional information\n",
        "print(f\"\\nüìä Batch consistency:\")\n",
        "print(f\"Position embedding added to batch 0, position 0: {(input_embeddings[0][0] - token_embeddings[0][0])[:5]}\")\n",
        "print(f\"Position embedding added to batch 7, position 0: {(input_embeddings[7][0] - token_embeddings[7][0])[:5]}\")\n",
        "print(f\"Are they the same? {torch.equal(input_embeddings[0][0] - token_embeddings[0][0], input_embeddings[7][0] - token_embeddings[7][0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Understanding the Embedding Lookup Process\n",
        "\n",
        "Let's dive deeper into how the embedding layer works. It's essentially a **lookup table operation** that retrieves rows from the embedding matrix using token IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the lookup process step by step\n",
        "print(\"üîç Step-by-step Embedding Lookup Process:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Create a simple example with small vocabulary\n",
        "simple_vocab_size = 6\n",
        "simple_emb_dim = 3\n",
        "torch.manual_seed(123)\n",
        "simple_embedding = nn.Embedding(simple_vocab_size, simple_emb_dim)\n",
        "\n",
        "print(f\"Simple embedding matrix shape: {simple_embedding.weight.shape}\")\n",
        "print(f\"Embedding matrix weights:\")\n",
        "print(simple_embedding.weight)\n",
        "\n",
        "# 2. Look up specific tokens\n",
        "test_token_ids = torch.tensor([2, 3, 4, 1])\n",
        "print(f\"\\nToken IDs to look up: {test_token_ids}\")\n",
        "\n",
        "# Method 1: Using the embedding layer directly\n",
        "embeddings_method1 = simple_embedding(test_token_ids)\n",
        "print(f\"\\nMethod 1 - Using embedding layer:\")\n",
        "print(f\"Result shape: {embeddings_method1.shape}\")\n",
        "print(f\"Embeddings:\\n{embeddings_method1}\")\n",
        "\n",
        "# Method 2: Manual lookup (equivalent to what happens internally)\n",
        "embeddings_method2 = simple_embedding.weight[test_token_ids]\n",
        "print(f\"\\nMethod 2 - Manual weight lookup:\")\n",
        "print(f\"Result shape: {embeddings_method2.shape}\")\n",
        "print(f\"Embeddings:\\n{embeddings_method2}\")\n",
        "\n",
        "# Verify they're the same\n",
        "print(f\"\\nAre both methods identical? {torch.equal(embeddings_method1, embeddings_method2)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ The embedding layer is just an efficient lookup table!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Creating a Complete GPT Embedding Module\n",
        "\n",
        "Let's now create a reusable module that combines both token and positional embeddings, similar to what we'll use in our complete GPT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined token and positional embedding layer for GPT-style models.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, context_size):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_emb = nn.Embedding(context_size, emb_dim)\n",
        "        \n",
        "    def forward(self, token_ids):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: Tensor of shape [batch_size, seq_len]\n",
        "        Returns:\n",
        "            Combined embeddings of shape [batch_size, seq_len, emb_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = token_ids.shape\n",
        "        \n",
        "        # Get token embeddings\n",
        "        tok_embeddings = self.tok_emb(token_ids)\n",
        "        \n",
        "        # Get positional embeddings\n",
        "        pos_ids = torch.arange(seq_len, device=token_ids.device)\n",
        "        pos_embeddings = self.pos_emb(pos_ids)\n",
        "        \n",
        "        # Combine them\n",
        "        return tok_embeddings + pos_embeddings\n",
        "\n",
        "# Test our complete embedding module\n",
        "embedding_module = GPTEmbedding(vocab_size, embedding_dim, context_size)\n",
        "\n",
        "# Test with the same data\n",
        "final_embeddings = embedding_module(inputs)\n",
        "\n",
        "print(\"üöÄ Complete GPT Embedding Module Test:\")\n",
        "print(f\"Input shape: {inputs.shape}\")\n",
        "print(f\"Output shape: {final_embeddings.shape}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in embedding_module.parameters()):,}\")\n",
        "\n",
        "# Verify it gives the same result as our manual approach\n",
        "manual_result = token_embeddings + pos_embeddings\n",
        "print(f\"\\nDoes our module match manual approach? {torch.allclose(final_embeddings, manual_result)}\")\n",
        "\n",
        "print(\"\\nüéâ Success! Our embedding layer is ready for the transformer!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary\n",
        "\n",
        "We have successfully created the first neural network layer of our LLM! \n",
        "\n",
        "**What we've accomplished:**\n",
        "- ‚úÖ Built token embeddings to capture semantic meaning\n",
        "- ‚úÖ Added positional embeddings to capture word order  \n",
        "- ‚úÖ Combined them into a complete input representation\n",
        "- ‚úÖ Created a reusable `GPTEmbedding` module\n",
        "\n",
        "**Key takeaways:**\n",
        "- Embeddings transform meaningless token IDs into rich, learnable vectors\n",
        "- Positional embeddings solve the \"bag of words\" problem \n",
        "- The embedding layer is just an efficient lookup table\n",
        "- Broadcasting makes it easy to combine different tensor shapes\n",
        "\n",
        "**What's next:** In Part 5, we'll build the self-attention mechanism that will process these embeddings and allow our model to understand relationships between words!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
