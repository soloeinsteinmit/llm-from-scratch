{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building LLMs From Scratch (Part 5): The Complete Data Preprocessing Pipeline\n",
        "\n",
        "Welcome to Part 5! This notebook demonstrates the complete end-to-end data preprocessing pipeline we've built over the previous parts. We'll see how tokenization, data loading, and embeddings work together to transform raw text into model-ready tensors.\n",
        "\n",
        "### üîó Quick Links\n",
        "- **Medium Article**: [Part 5: The Complete Data Preprocessing Pipeline](https://soloshun.medium.com/link-to-part-5)\n",
        "- **GitHub Repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
        "\n",
        "### üìã What We'll Cover\n",
        "1. **Step 1**: Tokenization with BPE (Byte Pair Encoding)\n",
        "2. **Step 2**: Creating Input-Target pairs with DataLoader\n",
        "3. **Step 3**: Token & Positional Embeddings\n",
        "4. **Step 4**: Complete Pipeline Integration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's import all the necessary libraries and our custom modules from previous parts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the system path to allow imports\n",
        "sys.path.insert(0, '../')\n",
        "\n",
        "from src.part03_dataloader import create_dataloader_v1\n",
        "from src.part04_embeddings import GPTEmbedding\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Raw Text Data\n",
        "\n",
        "First, let's load our text data. We'll use \"The Verdict\" text file that we've been working with throughout the series.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the raw text data\n",
        "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(f\"üìñ Loaded text data:\")\n",
        "print(f\"Total characters: {len(raw_text):,}\")\n",
        "print(f\"First 100 characters: '{raw_text[:100]}'\")\n",
        "print(f\"Last 100 characters: '{raw_text[-100:]}'\")\n",
        "\n",
        "# Show a sample of the text structure\n",
        "lines = raw_text.split('\\n')\n",
        "print(f\"\\nüìä Text structure:\")\n",
        "print(f\"Total lines: {len(lines)}\")\n",
        "print(f\"Average line length: {sum(len(line) for line in lines) / len(lines):.1f} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Tokenization with Byte Pair Encoding (BPE)\n",
        "\n",
        "Now we'll tokenize our text using OpenAI's GPT-2 tokenizer, which uses Byte Pair Encoding. This is the same tokenizer used in GPT-2, GPT-3, and GPT-4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Tokenize the entire text\n",
        "tokenized_text = tokenizer.encode(raw_text)\n",
        "\n",
        "print(f\"üî¢ Tokenization Results:\")\n",
        "print(f\"Original text length: {len(raw_text):,} characters\")\n",
        "print(f\"Tokenized length: {len(tokenized_text):,} tokens\")\n",
        "print(f\"Compression ratio: {len(raw_text) / len(tokenized_text):.2f} chars/token\")\n",
        "print(f\"Vocabulary size: {tokenizer.n_vocab:,}\")\n",
        "\n",
        "# Show some example tokens\n",
        "print(f\"\\nüîç First 20 tokens: {tokenized_text[:20]}\")\n",
        "print(f\"Decoded: '{tokenizer.decode(tokenized_text[:20])}'\")\n",
        "\n",
        "# Show individual token examples\n",
        "sample_tokens = tokenized_text[10:15]\n",
        "print(f\"\\nüìù Token breakdown:\")\n",
        "for i, token_id in enumerate(sample_tokens):\n",
        "    token_text = tokenizer.decode([token_id])\n",
        "    print(f\"  Token {i}: ID={token_id:5d} ‚Üí '{token_text}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Input-Target Pairs with DataLoader\n",
        "\n",
        "Now we'll use our custom DataLoader from Part 3 to create training examples. The DataLoader will create input-target pairs using a sliding window approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "BATCH_SIZE = 8\n",
        "CONTEXT_SIZE = 4  # Small for demonstration\n",
        "STRIDE = CONTEXT_SIZE  # No overlap for this demo\n",
        "\n",
        "# Create the DataLoader\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_length=CONTEXT_SIZE,\n",
        "    stride=STRIDE,\n",
        "    shuffle=False,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "print(f\"üìä DataLoader Configuration:\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Context size: {CONTEXT_SIZE}\")\n",
        "print(f\"Stride: {STRIDE}\")\n",
        "print(f\"Total batches: {len(dataloader)}\")\n",
        "print(f\"Total examples: {len(dataloader.dataset)}\")\n",
        "\n",
        "# Get one batch of data\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(f\"\\nüéØ Sample Batch:\")\n",
        "print(f\"Inputs shape: {inputs.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"\\nInputs (token IDs):\\n{inputs}\")\n",
        "print(f\"\\nTargets (token IDs):\\n{targets}\")\n",
        "\n",
        "# Show the relationship between inputs and targets\n",
        "print(f\"\\nüîç Input-Target Relationship (first example):\")\n",
        "input_tokens = inputs[0].tolist()\n",
        "target_tokens = targets[0].tolist()\n",
        "print(f\"Input:  {input_tokens}\")\n",
        "print(f\"Target: {target_tokens}\")\n",
        "print(f\"Notice: Target is Input shifted by 1 position ‚Üí\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Token & Positional Embeddings\n",
        "\n",
        "Now we'll convert our token IDs into dense vectors using our GPTEmbedding module from Part 4. This combines both token embeddings (semantic meaning) and positional embeddings (sequence order).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define embedding parameters\n",
        "VOCAB_SIZE = 50257  # GPT-2 vocabulary size\n",
        "EMB_DIM = 256       # Embedding dimension\n",
        "\n",
        "# Initialize the embedding layer\n",
        "torch.manual_seed(123)  # For reproducible results\n",
        "embedding_layer = GPTEmbedding(VOCAB_SIZE, EMB_DIM, CONTEXT_SIZE)\n",
        "\n",
        "print(f\"üß† Embedding Layer Configuration:\")\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE:,}\")\n",
        "print(f\"Embedding dimension: {EMB_DIM}\")\n",
        "print(f\"Context size: {CONTEXT_SIZE}\")\n",
        "\n",
        "# Calculate total parameters\n",
        "total_params = sum(p.numel() for p in embedding_layer.parameters())\n",
        "token_params = VOCAB_SIZE * EMB_DIM\n",
        "pos_params = CONTEXT_SIZE * EMB_DIM\n",
        "\n",
        "print(f\"\\nüìä Parameter Count:\")\n",
        "print(f\"Token embedding parameters: {token_params:,}\")\n",
        "print(f\"Positional embedding parameters: {pos_params:,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Convert token IDs to embeddings\n",
        "model_ready_inputs = embedding_layer(inputs)\n",
        "\n",
        "print(f\"\\n‚ú® Embedding Results:\")\n",
        "print(f\"Input shape (token IDs): {inputs.shape}\")\n",
        "print(f\"Output shape (embeddings): {model_ready_inputs.shape}\")\n",
        "print(f\"Each token ID ‚Üí {EMB_DIM}-dimensional vector with positional info\")\n",
        "\n",
        "# Show the transformation for one example\n",
        "print(f\"\\nüîç Transformation Example (first sample):\")\n",
        "print(f\"Token IDs: {inputs[0].tolist()}\")\n",
        "print(f\"Embedding shape: {model_ready_inputs[0].shape}\")\n",
        "print(f\"First embedding vector (first 10 values): {model_ready_inputs[0][0][:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Complete Pipeline Integration\n",
        "\n",
        "Let's now put everything together into a single, streamlined function that demonstrates the complete preprocessing pipeline from raw text to model-ready tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def complete_preprocessing_pipeline(raw_text, batch_size=8, context_size=4, emb_dim=256):\n",
        "    \"\"\"\n",
        "    Complete data preprocessing pipeline from raw text to model-ready tensors.\n",
        "    \n",
        "    Args:\n",
        "        raw_text: Raw input text\n",
        "        batch_size: Number of examples per batch\n",
        "        context_size: Length of each input sequence\n",
        "        emb_dim: Embedding dimension\n",
        "    \n",
        "    Returns:\n",
        "        model_ready_inputs: Tensor ready for transformer model [batch_size, context_size, emb_dim]\n",
        "        targets: Target tokens for training [batch_size, context_size]\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Running Complete Preprocessing Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Step 1: Create DataLoader (handles tokenization + input-target pairs)\n",
        "    dataloader = create_dataloader_v1(\n",
        "        raw_text,\n",
        "        batch_size=batch_size,\n",
        "        max_length=context_size,\n",
        "        stride=context_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "    print(f\"‚úÖ Step 1: DataLoader created ({len(dataloader)} batches)\")\n",
        "    \n",
        "    # Step 2: Initialize embedding layer\n",
        "    embedding_layer = GPTEmbedding(50257, emb_dim, context_size)\n",
        "    print(f\"‚úÖ Step 2: Embedding layer initialized ({sum(p.numel() for p in embedding_layer.parameters()):,} params)\")\n",
        "    \n",
        "    # Step 3: Get one batch and process it\n",
        "    data_iter = iter(dataloader)\n",
        "    inputs, targets = next(data_iter)\n",
        "    print(f\"‚úÖ Step 3: Batch loaded {inputs.shape}\")\n",
        "    \n",
        "    # Step 4: Convert to embeddings\n",
        "    model_ready_inputs = embedding_layer(inputs)\n",
        "    print(f\"‚úÖ Step 4: Embeddings created {model_ready_inputs.shape}\")\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "    print(\"üéâ Pipeline Complete!\")\n",
        "    \n",
        "    return model_ready_inputs, targets\n",
        "\n",
        "# Run the complete pipeline\n",
        "final_inputs, final_targets = complete_preprocessing_pipeline(\n",
        "    raw_text, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    context_size=CONTEXT_SIZE, \n",
        "    emb_dim=EMB_DIM\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Final Results:\")\n",
        "print(f\"Model-ready inputs: {final_inputs.shape}\")\n",
        "print(f\"Training targets: {final_targets.shape}\")\n",
        "print(f\"Ready for transformer model! üöÄ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary\n",
        "\n",
        "We have successfully built and demonstrated the complete data preprocessing pipeline for our LLM! \n",
        "\n",
        "### What We Accomplished:\n",
        "- ‚úÖ **Tokenization**: Used BPE (GPT-2 tokenizer) to convert text to token IDs\n",
        "- ‚úÖ **Data Loading**: Created input-target pairs with sliding window approach  \n",
        "- ‚úÖ **Embeddings**: Combined token and positional embeddings for rich representations\n",
        "- ‚úÖ **Batching**: Organized data for efficient training\n",
        "\n",
        "### The Journey:\n",
        "```\n",
        "Raw Text ‚Üí Tokenization ‚Üí Input/Target Pairs ‚Üí Embeddings ‚Üí Model-Ready Tensors\n",
        "```\n",
        "\n",
        "### Key Takeaways:\n",
        "- **BPE tokenization** efficiently handles any text with a manageable vocabulary\n",
        "- **Sliding window** creates thousands of training examples from a single text\n",
        "- **Embeddings** transform meaningless IDs into information-rich vectors\n",
        "- **Modular design** makes each component reusable and testable\n",
        "\n",
        "### What's Next:\n",
        "In **Part 6**, we'll build the **self-attention mechanism** - the heart of the transformer that will process these embeddings and learn to understand language!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
