{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cca624",
   "metadata": {},
   "source": [
    "# Building LLMs From Scratch (Part 7): Self-Attention with Trainable Weights\n",
    "\n",
    "Welcome to Part 7! In this notebook, we'll upgrade our simplified attention mechanism with **trainable weights**. We'll introduce the famous **Query, Key, and Value** matrices that allow the model to actually *learn* which relationships matter.\n",
    "\n",
    "### ðŸ”— Quick Links\n",
    "- **Medium Article**: [Part 7: Self-Attention with Trainable Weights](https://soloshun.medium.com/building-llms-from-scratch-part-7-self-attention-with-weights)\n",
    "- **GitHub Repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
    "\n",
    "### ðŸ“‹ What We'll Cover\n",
    "1. **The Missing Piece**: Why simplified attention can't learn\n",
    "2. **Query, Key, Value**: The three trainable projections\n",
    "3. **Scaled Dot-Product Attention**: Why we divide by sqrt(d_k)\n",
    "4. **Step-by-Step Implementation**: Building trainable attention\n",
    "5. **Two Implementations**: nn.Parameter vs nn.Linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c595aaf",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3d848",
   "metadata": {},
   "source": [
    "## Recap: The Goal of Attention\n",
    "\n",
    "> **The goal of self-attention is to convert `input vectors` into enriched `context vectors`.**\n",
    "\n",
    "In Part 6, we built a simplified attention mechanism that could compute context vectors based on similarity. But it had a critical limitation: **it couldn't learn**.\n",
    "\n",
    "The simplified version always computed attention the same way, based solely on the input embeddings. Today, we fix that by introducing **trainable weight matrices**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our example sentence\n",
    "sentence = \"Your journey starts with one step\"\n",
    "tokens = sentence.split()\n",
    "\n",
    "# Input embeddings (same as Part 6)\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55], # step\n",
    "])\n",
    "\n",
    "print(f\"ðŸ“ Input sentence: '{sentence}'\")\n",
    "print(f\"ðŸ”¢ Tokens: {tokens}\")\n",
    "print(f\"ðŸ§  Input shape: {inputs.shape}\")\n",
    "print(f\"\\nInput embeddings:\\n{inputs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81814ed2",
   "metadata": {},
   "source": [
    "## The Solution: Query, Key, and Value\n",
    "\n",
    "The breakthrough idea: Instead of using input embeddings directly, **project them into three different spaces** using trainable weight matrices:\n",
    "\n",
    "1. **Query (Q)**: \"What am I looking for?\"\n",
    "2. **Key (K)**: \"What do I contain?\"\n",
    "3. **Value (V)**: \"What information do I pass on?\"\n",
    "\n",
    "### The Search Engine Analogy\n",
    "\n",
    "Think of Google Search:\n",
    "- **Query**: The text you type (\"best Italian restaurants\")\n",
    "- **Key**: The index/metadata of web pages\n",
    "- **Value**: The actual content of the pages\n",
    "\n",
    "Google compares your **Query** to the **Keys** of all pages. If there's a strong match, it returns the corresponding **Value**.\n",
    "\n",
    "Self-attention works the same way!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4ae09",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Weight Matrices\n",
    "\n",
    "Let's create three trainable weight matrices: $W_q$, $W_k$, and $W_v$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "d_in = inputs.shape[1]  # Input dimension = 3\n",
    "d_out = 2               # Output dimension (we can project to a different size!)\n",
    "\n",
    "print(f\"ðŸ“ Dimensions:\")\n",
    "print(f\"  Input dimension (d_in): {d_in}\")\n",
    "print(f\"  Output dimension (d_out): {d_out}\")\n",
    "\n",
    "# Initialize weight matrices\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Note: requires_grad=False is just for this demo to keep values static\n",
    "# In real training, we'd set requires_grad=True (or just use the default)\n",
    "W_query = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "print(f\"\\nðŸ”§ Weight matrices initialized!\")\n",
    "print(f\"\\nW_query shape: {W_query.shape}\")\n",
    "print(f\"W_query:\\n{W_query}\")\n",
    "\n",
    "print(f\"\\nW_key shape: {W_key.shape}\")\n",
    "print(f\"W_key:\\n{W_key}\")\n",
    "\n",
    "print(f\"\\nW_value shape: {W_value.shape}\")\n",
    "print(f\"W_value:\\n{W_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ba3c3",
   "metadata": {},
   "source": [
    "## Step 2: Compute Query, Key, and Value\n",
    "\n",
    "Now we project our input embeddings into the Q, K, V spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1051d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project all inputs into Q, K, V\n",
    "queries = inputs @ W_query  # (6, 3) @ (3, 2) = (6, 2)\n",
    "keys    = inputs @ W_key    # (6, 3) @ (3, 2) = (6, 2)\n",
    "values  = inputs @ W_value  # (6, 3) @ (3, 2) = (6, 2)\n",
    "\n",
    "print(f\"ðŸ“Š Projections computed!\")\n",
    "print(f\"\\nQueries shape: {queries.shape}\")\n",
    "print(f\"Queries:\\n{queries}\")\n",
    "\n",
    "print(f\"\\nKeys shape: {keys.shape}\")\n",
    "print(f\"Keys:\\n{keys}\")\n",
    "\n",
    "print(f\"\\nValues shape: {values.shape}\")\n",
    "print(f\"Values:\\n{values}\")\n",
    "\n",
    "print(f\"\\nâœ¨ We've transformed our 3D embeddings into 2D Q, K, V representations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ddb97",
   "metadata": {},
   "source": [
    "## Step 3: Compute Attention Scores and Scale\n",
    "\n",
    "Let's compute attention for \"journey\" and apply scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on \"journey\" (index 1)\n",
    "query_idx = 1\n",
    "query_2 = queries[query_idx]\n",
    "\n",
    "print(f\"ðŸŽ¯ Computing attention for: '{tokens[query_idx]}'\")\n",
    "print(f\"Query vector: {query_2}\")\n",
    "\n",
    "# Compute attention scores\n",
    "attn_scores_2 = query_2 @ keys.T  # (2,) @ (2, 6) = (6,)\n",
    "\n",
    "print(f\"\\nðŸ“Š Attention scores (before scaling):\")\n",
    "for i, (token, score) in enumerate(zip(tokens, attn_scores_2)):\n",
    "    print(f\"  {token:>8}: {score:.4f}\")\n",
    "\n",
    "# Scale the scores\n",
    "d_k = keys.shape[-1]\n",
    "attn_scores_2_scaled = attn_scores_2 / (d_k ** 0.5)\n",
    "\n",
    "print(f\"\\nðŸ“ Scaling by sqrt({d_k}) = {d_k**0.5:.4f}\")\n",
    "print(f\"\\nðŸ“Š Attention scores (after scaling):\")\n",
    "for i, (token, score) in enumerate(zip(tokens, attn_scores_2_scaled)):\n",
    "    print(f\"  {token:>8}: {score:.4f}\")\n",
    "\n",
    "# Apply softmax\n",
    "attn_weights_2 = torch.softmax(attn_scores_2_scaled, dim=-1)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Attention weights:\")\n",
    "for i, (token, weight) in enumerate(zip(tokens, attn_weights_2)):\n",
    "    print(f\"  {token:>8}: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "\n",
    "# Compute context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "\n",
    "print(f\"\\nâœ¨ Context vector for '{tokens[query_idx]}': {context_vec_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d3b29",
   "metadata": {},
   "source": [
    "## Why Scale by sqrt(d_k)?\n",
    "\n",
    "Let's demonstrate why scaling is crucial for training stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reason 1: Preventing Softmax Saturation\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "print(\"ðŸ”¬ Softmax Saturation Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original scores: {tensor}\")\n",
    "\n",
    "# Without scaling\n",
    "softmax_normal = torch.softmax(tensor, dim=-1)\n",
    "print(f\"\\nSoftmax (normal): {softmax_normal}\")\n",
    "print(f\"Distribution: Relatively balanced\")\n",
    "\n",
    "# With large scaling (simulating large d_k without correction)\n",
    "scaled_tensor = tensor * 8\n",
    "softmax_scaled = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(f\"\\nScaled scores (Ã—8): {scaled_tensor}\")\n",
    "print(f\"Softmax (scaled Ã—8): {softmax_scaled}\")\n",
    "print(f\"Distribution: Extremely peaky! (one value dominates)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ When scores are too large, softmax becomes overconfident.\")\n",
    "print(f\"   This causes vanishing gradients and unstable training.\")\n",
    "print(f\"   Dividing by sqrt(d_k) prevents this problem!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3cbe05",
   "metadata": {},
   "source": [
    "## Computing for All Tokens at Once\n",
    "\n",
    "Let's use matrix operations to compute attention for all tokens simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Computing attention for all tokens simultaneously:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute all attention scores\n",
    "attn_scores = queries @ keys.T  # (6, 2) @ (2, 6) = (6, 6)\n",
    "print(f\"ðŸ“Š Attention scores shape: {attn_scores.shape}\")\n",
    "\n",
    "# Scale\n",
    "attn_scores_scaled = attn_scores / (d_k ** 0.5)\n",
    "\n",
    "# Apply softmax\n",
    "attn_weights = torch.softmax(attn_scores_scaled, dim=-1)\n",
    "print(f\"ðŸŽ¯ Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights:\\n{attn_weights}\")\n",
    "\n",
    "# Verify our manual calculation\n",
    "print(f\"\\nâœ… Verification - 'journey' attention weights:\")\n",
    "print(f\"Manual: {attn_weights_2}\")\n",
    "print(f\"Matrix: {attn_weights[1]}\")\n",
    "print(f\"Match: {torch.allclose(attn_weights_2, attn_weights[1])}\")\n",
    "\n",
    "# Compute all context vectors\n",
    "all_context_vectors = attn_weights @ values  # (6, 6) @ (6, 2) = (6, 2)\n",
    "print(f\"\\nâœ¨ All context vectors shape: {all_context_vectors.shape}\")\n",
    "print(f\"All context vectors:\\n{all_context_vectors}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1717e9f",
   "metadata": {},
   "source": [
    "## Implementation 1: Using nn.Parameter\n",
    "\n",
    "Let's wrap our attention mechanism in a clean class using `nn.Parameter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    \"\"\"Self-attention with trainable weights using nn.Parameter.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_scores = attn_scores / keys.shape[-1]**0.5\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ§ª Testing SelfAttention_v1:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "output_v1 = sa_v1(inputs)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Output shape: {output_v1.shape}\")\n",
    "print(f\"\\nOutput:\\n{output_v1}\")\n",
    "\n",
    "# Verify it matches our manual calculation\n",
    "print(f\"\\nâœ… Matches manual calculation: {torch.allclose(output_v1, all_context_vectors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc1dd3",
   "metadata": {},
   "source": [
    "## Implementation 2: Using nn.Linear (Preferred)\n",
    "\n",
    "In practice, we use `nn.Linear` instead of `nn.Parameter`. It's cleaner, more efficient, and handles initialization automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    \"\"\"Self-attention with trainable weights using nn.Linear.\n",
    "    \n",
    "    This is the preferred implementation - used in GPT-2, BERT, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ§ª Testing SelfAttention_v2:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out, qkv_bias=False)\n",
    "output_v2 = sa_v2(inputs)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Output shape: {output_v2.shape}\")\n",
    "print(f\"\\nOutput:\\n{output_v2}\")\n",
    "\n",
    "print(f\"\\nâœ¨ This is the foundation of GPT-2 and other transformers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79145cc7",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary and Key Takeaways\n",
    "\n",
    "We've successfully implemented self-attention with trainable weights!\n",
    "\n",
    "### What We Accomplished:\n",
    "- âœ… **Introduced Trainable Weights**: Q, K, V matrices that the model can learn\n",
    "- âœ… **Understood the Analogy**: Query (search), Key (index), Value (content)\n",
    "- âœ… **Implemented Scaling**: Divided by sqrt(d_k) for stability\n",
    "- âœ… **Built Two Versions**: nn.Parameter (educational) and nn.Linear (practical)\n",
    "\n",
    "### The Five-Step Process:\n",
    "1. **Project into Q, K, V**: Multiply inputs by trainable weight matrices\n",
    "2. **Compute Attention Scores**: Dot product between queries and keys\n",
    "3. **Scale**: Divide by sqrt(d_k) for stability\n",
    "4. **Normalize**: Apply softmax to get attention weights\n",
    "5. **Weighted Sum**: Multiply attention weights by values\n",
    "\n",
    "### Key Insights:\n",
    "- **Trainable weights** allow the model to learn optimal attention patterns\n",
    "- **Query** determines what to look for\n",
    "- **Key** represents what content is available\n",
    "- **Value** contains the actual information to retrieve\n",
    "- **Scaling** prevents softmax saturation and stabilizes variance\n",
    "- **nn.Linear** is preferred over nn.Parameter in practice\n",
    "\n",
    "### What's Next:\n",
    "In **Part 8**, we'll implement **Causal Attention** (Masked Self-Attention) to ensure the model can only look at previous tokens, not future ones. This is crucial for text generation!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”— Resources\n",
    "- **Medium Article**: [Part 7: Self-Attention with Trainable Weights](https://soloshun.medium.com/building-llms-from-scratch-part-7-self-attention-with-weights)\n",
    "- **GitHub Repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
    "- **Python Script**: `src/part07_self_attention_with_weights.py`\n",
    "\n",
    "### ðŸ”œ Coming Next\n",
    "**Part 8: Causal Attention (Masked Self-Attention)**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
