{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbbe569",
   "metadata": {},
   "source": [
    "# Building LLMs From Scratch (Part 8): Causal Attention\n",
    "\n",
    "Welcome to Part 8! In this notebook, we'll implement **Causal Attention** (also known as Masked Self-Attention). This prevents the model from \"cheating\" by looking at future tokens, which is essential for autoregressive text generation.\n",
    "\n",
    "### üîó Quick Links\n",
    "- **Medium Article**: [Part 8: Causal Attention](https://soloshun.medium.com/building-llms-from-scratch-part-8-causal-attention)\n",
    "- **GitHub Repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
    "\n",
    "### üìã What We'll Cover\n",
    "1. **The Problem**: Why standard self-attention \"cheats\"\n",
    "2. **The Solution**: Diagonal masking to hide future tokens\n",
    "3. **Implementation**: Why we mask BEFORE softmax\n",
    "4. **Causal Attention Class**: Production-ready implementation\n",
    "5. **Dropout**: Regularization in attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83074fd",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074abb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a461c",
   "metadata": {},
   "source": [
    "## The Problem: Future Leakage\n",
    "\n",
    "Let's set up our example and see the problem with standard self-attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55], # step\n",
    "])\n",
    "\n",
    "d_in = inputs.shape[-1]  # embedding dimension = 3\n",
    "d_out = 2  # output dimension\n",
    "\n",
    "print(f\"üìù Sentence: '{' '.join(words)}'\")\n",
    "print(f\"üß† Input shape: {inputs.shape}\")\n",
    "print(f\"üìê d_in={d_in}, d_out={d_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00233e44",
   "metadata": {},
   "source": [
    "Let's create a standard self-attention mechanism (from Part 7) to see the problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Test it\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "output = sa_v2(inputs)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebac748",
   "metadata": {},
   "source": [
    "Now let's look at the attention weights to see the problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d70b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "print(\"Attention Weights (each row sums to 1):\")\n",
    "print(attn_weights)\n",
    "print(f\"\\n‚ùå Problem: Token 'journey' (row 1) can see 'starts', 'with', 'one', 'step'\")\n",
    "print(\"   These are FUTURE tokens that shouldn't exist yet during generation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13837786",
   "metadata": {},
   "source": [
    "## The Solution: Causal Masking\n",
    "\n",
    "To fix this, we need to create a **mask** that prevents tokens from attending to future positions. We use a **lower triangular mask** where:\n",
    "- **0** = allowed (past and current tokens)\n",
    "- **1** = masked (future tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a causal mask using torch.triu (triangular upper)\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "\n",
    "print(\"Lower Triangular Mask (1 = keep, 0 = hide):\")\n",
    "print(mask_simple)\n",
    "print(f\"\\n‚úÖ Each row can only see tokens up to and including its position\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a761f61",
   "metadata": {},
   "source": [
    "### Approach 1: Mask After Softmax (WRONG ‚ùå)\n",
    "\n",
    "Let's first try the naive approach of masking AFTER softmax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45505bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply attention weights by mask\n",
    "masked_simple = attn_weights * mask_simple\n",
    "\n",
    "print(\"After masking (but rows don't sum to 1):\")\n",
    "print(masked_simple)\n",
    "\n",
    "# Need to renormalize\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "\n",
    "print(\"\\nAfter renormalization:\")\n",
    "print(masked_simple_norm)\n",
    "\n",
    "print(\"\\n‚ùå Problem: The softmax denominator included future tokens!\")\n",
    "print(\"   This causes subtle DATA LEAKAGE. The probabilities of past tokens\")\n",
    "print(\"   were influenced by the presence of future tokens.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09637df",
   "metadata": {},
   "source": [
    "### Approach 2: Mask Before Softmax (CORRECT ‚úÖ)\n",
    "\n",
    "The correct way is to mask BEFORE applying softmax. We replace future positions with `-inf`, which becomes 0 after softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda747c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create upper triangular mask (1 = hide, 0 = keep)\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "print(\"Upper Triangular Mask (1 = hide, 0 = keep):\")\n",
    "print(mask)\n",
    "\n",
    "# Replace masked positions with -inf\n",
    "masked_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "print(\"\\nAttention Scores after masking with -inf:\")\n",
    "print(masked_scores)\n",
    "\n",
    "# Apply softmax\n",
    "attn_weights_correct = torch.softmax(masked_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "print(\"\\nAttention Weights after softmax:\")\n",
    "print(attn_weights_correct)\n",
    "\n",
    "print(\"\\n‚úÖ Correct! Future tokens had NO influence on the probabilities.\")\n",
    "print(\"   Each row properly sums to 1.0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253163c",
   "metadata": {},
   "source": [
    "## Dropout in Attention\n",
    "\n",
    "**Dropout** is a regularization technique that randomly zeros out a fraction of values during training to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = nn.Dropout(0.5)\n",
    "\n",
    "# Example with simple tensor\n",
    "example = torch.ones(6, 6)\n",
    "print(\"Original tensor:\")\n",
    "print(example)\n",
    "\n",
    "dropped = dropout(example)\n",
    "print(f\"\\nAfter dropout (p=0.5):\")\n",
    "print(dropped)\n",
    "\n",
    "print(\"\\nüí° Key points:\")\n",
    "print(\"- Dropout randomly zeros out 50% of values\")\n",
    "print(\"- Remaining values are scaled by 2x (1/(1-p))\")\n",
    "print(\"- This prevents overfitting during training\")\n",
    "print(\"- During inference, dropout is automatically disabled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8ecfa",
   "metadata": {},
   "source": [
    "Now let's apply dropout to our attention weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "dropped_weights = dropout(attn_weights_correct)\n",
    "\n",
    "print(\"Attention weights after dropout:\")\n",
    "print(dropped_weights)\n",
    "print(\"\\n‚úÖ Some attention weights are now zero!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188e17d",
   "metadata": {},
   "source": [
    "## Building the CausalAttention Class\n",
    "\n",
    "Now let's put everything together into a production-ready PyTorch module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention with trainable weights and masking.\n",
    "    \n",
    "    Prevents tokens from attending to future positions,\n",
    "    which is essential for autoregressive language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Register causal mask as a buffer\n",
    "        # (non-trainable, but moves with model and gets saved/loaded)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with causal masking.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_tokens, d_in)\n",
    "            \n",
    "        Returns:\n",
    "            context_vectors: Output of shape (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # Note: transpose(1, 2) swaps the sequence and embedding dimensions\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        \n",
    "        # Apply causal mask (hide future tokens)\n",
    "        # Slice mask to match current sequence length\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        \n",
    "        # Scale and apply softmax\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        # Apply dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Compute context vectors\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "print(\"‚úÖ CausalAttention class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf36fb1",
   "metadata": {},
   "source": [
    "### Understanding `register_buffer`\n",
    "\n",
    "The `register_buffer` method is crucial for our mask. Let's understand why:\n",
    "\n",
    "**What it does:**\n",
    "- Tells PyTorch: \"This tensor is part of the module's state, but NOT a trainable parameter\"\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Saved and loaded with `model.state_dict()`\n",
    "- ‚úÖ Automatically moved to GPU/CPU with `model.to(device)`\n",
    "- ‚ùå NOT updated during backpropagation (no gradients)\n",
    "\n",
    "**Why we need it:**\n",
    "- The causal mask is fixed for a given context length\n",
    "- It doesn't change during training\n",
    "- But it needs to follow the model when we save/load or move to GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc3b21",
   "metadata": {},
   "source": [
    "## Testing CausalAttention\n",
    "\n",
    "Let's test our causal attention with a single sequence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "# Create causal attention\n",
    "ca = CausalAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=len(words),\n",
    "    dropout=0.0  # No dropout for testing\n",
    ")\n",
    "\n",
    "# Add batch dimension\n",
    "inputs_batched = inputs.unsqueeze(0)\n",
    "\n",
    "# Forward pass\n",
    "context_vecs = ca(inputs_batched)\n",
    "\n",
    "print(f\"Input shape: {inputs_batched.shape}\")\n",
    "print(f\"Output shape: {context_vecs.shape}\")\n",
    "print(f\"\\nContext vectors:\\n{context_vecs[0]}\")\n",
    "print(\"\\n‚úÖ Causal attention working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc293b",
   "metadata": {},
   "source": [
    "## Testing with Batches\n",
    "\n",
    "Modern deep learning processes multiple sequences at once. Let's test with a batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch (duplicate the inputs for demonstration)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"  - Batch size: {batch.shape[0]}\")\n",
    "print(f\"  - Sequence length: {batch.shape[1]}\")\n",
    "print(f\"  - Embedding dim: {batch.shape[2]}\")\n",
    "\n",
    "# Apply causal attention\n",
    "torch.manual_seed(123)\n",
    "ca_batch = CausalAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=batch.shape[1],\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "context_vecs_batch = ca_batch(batch)\n",
    "\n",
    "print(f\"\\nOutput shape: {context_vecs_batch.shape}\")\n",
    "print(f\"Output:\\n{context_vecs_batch}\")\n",
    "\n",
    "print(\"\\n‚úÖ Causal attention handles batches seamlessly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af713b36",
   "metadata": {},
   "source": [
    "## Visualizing the Mask\n",
    "\n",
    "Let's visualize how the causal mask works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29387ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "context_length = 6\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Mask\n",
    "im1 = ax1.imshow(mask.numpy(), cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "ax1.set_title('Causal Mask\\n(0 = allowed, 1 = masked)', fontsize=14, pad=20)\n",
    "ax1.set_xlabel('Key Position (attending TO)')\n",
    "ax1.set_ylabel('Query Position (attending FROM)')\n",
    "\n",
    "for i in range(context_length):\n",
    "    for j in range(context_length):\n",
    "        text = ax1.text(j, i, 'X' if mask[i, j] == 1 else '‚úì',\n",
    "                      ha=\"center\", va=\"center\", color=\"white\", \n",
    "                      fontsize=16, weight='bold')\n",
    "\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "\n",
    "# Plot 2: Attention weights after masking\n",
    "torch.manual_seed(123)\n",
    "sample_scores = torch.randn(context_length, context_length)\n",
    "masked_scores = sample_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "attn_weights_vis = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "im2 = ax2.imshow(attn_weights_vis.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "ax2.set_title('Causal Attention Weights\\n(after masking & softmax)', fontsize=14, pad=20)\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "\n",
    "for i in range(context_length):\n",
    "    for j in range(context_length):\n",
    "        text = ax2.text(j, i, f'{attn_weights_vis[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\",\n",
    "                      color=\"white\" if attn_weights_vis[i, j] > 0.5 else \"black\",\n",
    "                      fontsize=10)\n",
    "\n",
    "plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e808e3",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### üéØ What We Learned\n",
    "\n",
    "1. **The Problem**: Standard self-attention allows tokens to see future tokens, causing data leakage during training and failure during generation.\n",
    "\n",
    "2. **The Solution**: Causal masking prevents tokens from attending to future positions by:\n",
    "   - Creating an upper triangular mask\n",
    "   - Replacing future positions with `-inf` BEFORE softmax\n",
    "   - The softmax then naturally converts these to zero\n",
    "\n",
    "3. **Implementation Details**:\n",
    "   - Use `torch.triu()` to create the upper triangular mask\n",
    "   - Use `masked_fill()` to replace masked positions with `-inf`\n",
    "   - Use `register_buffer()` to store the mask with the model\n",
    "   - Use `transpose(1, 2)` for batch matrix multiplication\n",
    "\n",
    "4. **Dropout**: Regularization technique that randomly zeros out attention weights during training to prevent overfitting.\n",
    "\n",
    "### ‚úÖ Our Implementation Now Has\n",
    "\n",
    "- ‚úÖ Trainable Weights (Q, K, V)\n",
    "- ‚úÖ Scaled Dot-Product Attention\n",
    "- ‚úÖ Causal Masking\n",
    "- ‚úÖ Dropout\n",
    "- ‚úÖ Batch Support\n",
    "\n",
    "### üîú What's Next?\n",
    "\n",
    "In **Part 9**, we'll implement **Multi-Head Attention**, which runs multiple causal attention mechanisms in parallel to capture different types of relationships!\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- **Medium Article**: [Building LLMs From Scratch (Part 8): Causal Attention](https://soloshun.medium.com/building-llms-from-scratch-part-8-causal-attention)\n",
    "- **GitHub**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
    "- **Previous Part**: [Part 7: Self-Attention with Trainable Weights](https://medium.com/@soloshun/building-llms-from-scratch-part-7-self-attention-with-trainable-weights)\n",
    "\n",
    "---\n",
    "\n",
    "Thank you for following along! üôè\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930f73c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
