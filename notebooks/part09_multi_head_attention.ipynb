{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f733c7a2",
   "metadata": {},
   "source": [
    "# Building LLMs From Scratch (Part 9): Multi-Head Attention\n",
    "\n",
    "Welcome to Part 9! In this notebook, we'll implement **Multi-Head Attention**, the production-ready attention mechanism used in modern Transformers like GPT-4 and Llama.\n",
    "\n",
    "### üîó Quick Links\n",
    "- **Medium Article**: [Part 9: Multi-Head Attention](https://medium.com/@soloshun/building-llms-from-scratch-part-9-multi-head-attention)\n",
    "- **GitHub Repository**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
    "\n",
    "### üìã What We'll Cover\n",
    "1. **The Concept**: Why we need multiple heads\n",
    "2. **Two Approaches**: Wrapper vs. Efficient Weight Split\n",
    "3. **The Dimensions**: Understanding `d_out`, `num_heads`, and `head_dim`\n",
    "4. **Implementation**: Building `MultiHeadAttention` in PyTorch\n",
    "5. **Shape Tracing**: Step-by-step tensor transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7c3c4",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded522e8",
   "metadata": {},
   "source": [
    "## The Concept: Why Multiple Heads?\n",
    "\n",
    "Imagine reading a complex sentence. You might need to track:\n",
    "1. **Grammar**: Which noun corresponds to this verb?\n",
    "2. **Sentiment**: Is this sentence positive or negative?\n",
    "3. **Facts**: What specific entities are mentioned?\n",
    "\n",
    "A single attention head can only learn one type of relationship. **Multi-Head Attention** allows the model to learn multiple types of relationships in parallel!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f867dd",
   "metadata": {},
   "source": [
    "## Setup: CausalAttention (from Part 8)\n",
    "\n",
    "First, let's bring in our `CausalAttention` class from Part 8. We'll use it as a building block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"Single-head causal attention from Part 8\"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "print(\"‚úÖ CausalAttention class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c4d35",
   "metadata": {},
   "source": [
    "## Approach 1: The Wrapper (Naive)\n",
    "\n",
    "The simplest way to implement multi-head attention is to create multiple `CausalAttention` instances and concatenate their outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cae0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention using the Wrapper approach.\n",
    "    \n",
    "    Pros: Easy to understand\n",
    "    Cons: Less efficient (many small matrix operations)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Run each head independently and concatenate\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "print(\"‚úÖ MultiHeadAttentionWrapper class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542da58",
   "metadata": {},
   "source": [
    "Let's test the wrapper approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],\n",
    "    [0.55, 0.87, 0.66],\n",
    "    [0.57, 0.85, 0.64],\n",
    "    [0.22, 0.58, 0.33],\n",
    "    [0.77, 0.25, 0.10],\n",
    "    [0.05, 0.80, 0.55],\n",
    "])\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "print(f\"Input shape: {batch.shape}\")\n",
    "print(f\"  - Batch size: {batch.shape[0]}\")\n",
    "print(f\"  - Sequence length: {batch.shape[1]}\")\n",
    "print(f\"  - Embedding dim: {batch.shape[2]}\")\n",
    "\n",
    "# Create wrapper multi-head attention\n",
    "torch.manual_seed(123)\n",
    "d_in, d_out = 3, 2\n",
    "num_heads = 2\n",
    "\n",
    "mha_wrapper = MultiHeadAttentionWrapper(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=batch.shape[1],\n",
    "    dropout=0.0,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "context_vecs = mha_wrapper(batch)\n",
    "\n",
    "print(f\"\\nOutput shape: {context_vecs.shape}\")\n",
    "print(f\"  - Note: d_out ({d_out}) √ó num_heads ({num_heads}) = {d_out * num_heads}\")\n",
    "print(f\"\\nOutput:\\n{context_vecs}\")\n",
    "\n",
    "print(\"\\n‚úÖ Wrapper approach: Each head processes independently,\")\n",
    "print(\"   then outputs are concatenated!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ca32b",
   "metadata": {},
   "source": [
    "## Approach 2: Weight Splits (Efficient)\n",
    "\n",
    "The production-ready approach used in PyTorch, TensorFlow, and all modern Transformers. Instead of separate layers, we:\n",
    "1. Create ONE large set of Q, K, V weights\n",
    "2. Reshape to split into multiple heads\n",
    "3. Process all heads in parallel\n",
    "4. Concatenate heads back together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e473b2d",
   "metadata": {},
   "source": [
    "### Understanding the Dimensions\n",
    "\n",
    "Before coding, let's clarify the math:\n",
    "- `d_in` = Input embedding dimension\n",
    "- `d_out` = Total output dimension\n",
    "- `num_heads` = Number of attention heads\n",
    "- `head_dim` = `d_out / num_heads`\n",
    "\n",
    "Example: If `d_out=6` and `num_heads=2`, then `head_dim=3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272210d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0cc41e6",
   "metadata": {},
   "source": [
    "Let's test the efficient implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b927b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "d_in, d_out = 3, 6\n",
    "num_heads = 2\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=batch.shape[1],\n",
    "    dropout=0.0,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(f\"Input shape: {batch.shape}\")\n",
    "print(f\"Output shape: {context_vecs.shape}\")\n",
    "print(f\"  - d_out remains {d_out}\")\n",
    "print(f\"  - head_dim = d_out / num_heads = {d_out} / {num_heads} = {d_out // num_heads}\")\n",
    "print(f\"\\nOutput:\\n{context_vecs}\")\n",
    "\n",
    "print(\"\\n‚úÖ Efficient approach works!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4f42a",
   "metadata": {},
   "source": [
    "## Shape Tracing\n",
    "\n",
    "Let's trace the tensor transformations step by step to understand exactly what's happening:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "b, num_tokens, d_in = 1, 3, 6\n",
    "d_out = 6\n",
    "num_heads = 2\n",
    "head_dim = d_out // num_heads\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Batch size: {b}\")\n",
    "print(f\"  - Sequence length: {num_tokens}\")\n",
    "print(f\"  - Input dimension: {d_in}\")\n",
    "print(f\"  - Output dimension: {d_out}\")\n",
    "print(f\"  - Number of heads: {num_heads}\")\n",
    "print(f\"  - Head dimension: {head_dim}\")\n",
    "\n",
    "# Create dummy input\n",
    "x = torch.randn(b, num_tokens, d_in)\n",
    "\n",
    "# Create model\n",
    "torch.manual_seed(42)\n",
    "mha = MultiHeadAttention(d_in, d_out, num_tokens, 0.0, num_heads)\n",
    "\n",
    "# Manual forward pass with shape printing\n",
    "print(f\"\\nüìä Step-by-Step Shape Transformations:\")\n",
    "print(f\"1. Input:           {tuple(x.shape)}\")\n",
    "\n",
    "queries = mha.W_query(x)\n",
    "print(f\"2. After Linear:    {tuple(queries.shape)}\")\n",
    "\n",
    "queries = queries.view(b, num_tokens, num_heads, head_dim)\n",
    "print(f\"3. After Reshape:   {tuple(queries.shape)}\")\n",
    "\n",
    "queries = queries.transpose(1, 2)\n",
    "print(f\"4. After Transpose: {tuple(queries.shape)}\")\n",
    "\n",
    "# Simulate attention scores\n",
    "keys = mha.W_key(x).view(b, num_tokens, num_heads, head_dim).transpose(1, 2)\n",
    "attn_scores = queries @ keys.transpose(2, 3)\n",
    "print(f\"5. Attention Scores: {tuple(attn_scores.shape)}\")\n",
    "print(f\"   (Last 2 dims are {num_tokens}√ó{num_tokens} attention matrix)\")\n",
    "\n",
    "# Complete forward pass\n",
    "output = mha(x)\n",
    "print(f\"6. Final Output:    {tuple(output.shape)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Shape preserved! Input {tuple(x.shape)} ‚Üí Output {tuple(output.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef8b90",
   "metadata": {},
   "source": [
    "## Visualizing Multi-Head Attention\n",
    "\n",
    "Let's visualize how different heads might learn different patterns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6084ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a simple example\n",
    "torch.manual_seed(42)\n",
    "seq_len = 6\n",
    "d_in = 8\n",
    "d_out = 8\n",
    "num_heads = 2\n",
    "\n",
    "x = torch.randn(1, seq_len, d_in)\n",
    "mha = MultiHeadAttention(d_in, d_out, seq_len, 0.0, num_heads)\n",
    "\n",
    "# Get attention weights (we'll extract them manually)\n",
    "b, num_tokens, _ = x.shape\n",
    "keys = mha.W_key(x).view(b, num_tokens, num_heads, d_out // num_heads).transpose(1, 2)\n",
    "queries = mha.W_query(x).view(b, num_tokens, num_heads, d_out // num_heads).transpose(1, 2)\n",
    "\n",
    "attn_scores = queries @ keys.transpose(2, 3)\n",
    "mask_bool = mha.mask.bool()[:num_tokens, :num_tokens]\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    weights = attn_weights[0, head_idx].detach().numpy()\n",
    "    \n",
    "    im = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {head_idx + 1} Attention Weights', fontsize=14)\n",
    "    ax.set_xlabel('Key Position (attending TO)')\n",
    "    ax.set_ylabel('Query Position (attending FROM)')\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            text = ax.text(j, i, f'{weights[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\",\n",
    "                          color=\"white\" if weights[i, j] > 0.5 else \"black\",\n",
    "                          fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Notice how each head learns different attention patterns!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ee69c",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the wrapper approach vs. the efficient approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2695b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Setup\n",
    "batch_size = 8\n",
    "seq_len = 128\n",
    "d_in = 512\n",
    "d_out_per_head = 64\n",
    "num_heads = 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_in)\n",
    "\n",
    "# Wrapper approach\n",
    "print(\"Testing Wrapper Approach...\")\n",
    "torch.manual_seed(123)\n",
    "wrapper = MultiHeadAttentionWrapper(d_in, d_out_per_head, seq_len, 0.0, num_heads)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = wrapper(x)\n",
    "wrapper_time = time.time() - start\n",
    "\n",
    "# Efficient approach\n",
    "print(\"Testing Efficient Approach...\")\n",
    "torch.manual_seed(123)\n",
    "efficient = MultiHeadAttention(d_in, d_out_per_head * num_heads, seq_len, 0.0, num_heads)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = efficient(x)\n",
    "efficient_time = time.time() - start\n",
    "\n",
    "print(f\"\\nüìä Results (10 forward passes):\")\n",
    "print(f\"  Wrapper Approach:   {wrapper_time:.4f}s\")\n",
    "print(f\"  Efficient Approach: {efficient_time:.4f}s\")\n",
    "print(f\"  Speedup: {wrapper_time / efficient_time:.2f}x\")\n",
    "\n",
    "print(f\"\\n‚úÖ The efficient approach is significantly faster!\")\n",
    "print(\"   This is why all production Transformers use it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8fb7f",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### üéØ What We Learned\n",
    "\n",
    "1. **Why Multi-Head?**: Single-head attention can only learn one type of relationship. Multiple heads allow the model to learn different aspects (grammar, sentiment, facts, etc.) in parallel.\n",
    "\n",
    "2. **Two Approaches**:\n",
    "   - **Wrapper**: Easy to understand, but slower (many small matrix operations)\n",
    "   - **Weight Split**: Production-ready, much faster (single large matrix operation)\n",
    "\n",
    "3. **The Math**: \n",
    "   - `head_dim = d_out / num_heads`\n",
    "   - Example: `d_out=6`, `num_heads=2` ‚Üí `head_dim=3`\n",
    "\n",
    "4. **The Transformation**: \n",
    "   - Input: `(batch, tokens, d_in)`\n",
    "   - Linear: `(batch, tokens, d_out)`\n",
    "   - Reshape: `(batch, tokens, num_heads, head_dim)`\n",
    "   - Transpose: `(batch, num_heads, tokens, head_dim)`\n",
    "   - Attention: Process all heads in parallel\n",
    "   - Combine: `(batch, tokens, d_out)`\n",
    "\n",
    "### ‚úÖ Our Implementation Now Has\n",
    "\n",
    "- ‚úÖ Input Embeddings\n",
    "- ‚úÖ Positional Encodings\n",
    "- ‚úÖ **Causal Multi-Head Attention** ‚Üê We are here!\n",
    "- ‚è≠Ô∏è Dropout & Layer Normalization (coming soon)\n",
    "- ‚è≠Ô∏è Feed Forward Networks (coming soon)\n",
    "\n",
    "### üîú What's Next?\n",
    "\n",
    "In **Part 10**, we'll zoom out and take a **Bird's Eye View of the LLM Architecture**. We'll see how all these pieces (embeddings, attention, feedforward) fit together to form the complete Transformer!\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- **Medium Article**: [Building LLMs From Scratch (Part 9): Multi-Head Attention](https://medium.com/@soloshun/building-llms-from-scratch-part-9-multi-head-attention)\n",
    "- **GitHub**: [llm-from-scratch](https://github.com/soloeinsteinmit/llm-from-scratch)\n",
    "- **Previous Part**: [Part 8: Causal Attention](https://medium.com/@soloshun/building-llms-from-scratch-part-8-causal-attention)\n",
    "\n",
    "---\n",
    "\n",
    "Thank you for following along! üôè\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
